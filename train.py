import os
import glob
import logging
import argparse
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, WeightedRandomSampler
from torch.utils.tensorboard import SummaryWriter
from torch.amp import autocast, GradScaler # torch 2.0+
from datetime import datetime
from collections import Counter
from tqdm import tqdm
import psutil
from model.vis_mfn import VisMFN
from model.loss import create_loss_function
from dataset import VisibilityDataset, collate_fn_filter_none
from utils import config


def set_seed(seed=42):
    """
    è®¾ç½®å…¨å±€éšæœºç§å­ä»¥ç¡®ä¿è®­ç»ƒçš„å¯é‡å¤æ€§
    
    Args:
        seed (int): éšæœºç§å­å€¼ï¼Œé»˜è®¤ä¸º42
    """
    # Pythonå†…ç½®éšæœºæ•°
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    os.environ['PYTHONHASHSEED'] = str(seed)
    

def worker_init_fn():
    """
    DataLoader workeråˆå§‹åŒ–å‡½æ•°ï¼Œç¡®ä¿æ¯ä¸ªworkeræœ‰ä¸åŒä½†ç¡®å®šçš„éšæœºç§å­
    """
    # è·å–ä¸»è¿›ç¨‹çš„éšæœºç§å­
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)


def setup_logging(output_dir):
    """
    è®¾ç½®æ—¥å¿—è®°å½•
    """
    os.makedirs(output_dir, exist_ok=True)
    log_file = os.path.join(output_dir, f"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    
    # æ¸…é™¤å·²æœ‰çš„handlersï¼Œé¿å…é‡å¤æ—¥å¿—
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    # è®¾ç½®æ—¥å¿—æ ¼å¼
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    
    # æ–‡ä»¶å¤„ç†å™¨ - å¼ºåˆ¶ç«‹å³åˆ·æ–°
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(formatter)
    file_handler.setLevel(logging.INFO)
    
    # æ§åˆ¶å°å¤„ç†å™¨ - å¼ºåˆ¶ç«‹å³åˆ·æ–°
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    console_handler.setLevel(logging.INFO)
    
    # é…ç½®æ ¹logger
    logging.basicConfig(level=logging.INFO, handlers=[file_handler, console_handler])
    logger = logging.getLogger(__name__)
    
    # å¼ºåˆ¶ç«‹å³åˆ·æ–°æ‰€æœ‰æ—¥å¿—
    for handler in logger.handlers:
        if hasattr(handler, 'flush'):
            handler.flush()
    
    return logger


def get_memory_usage():
    """
    è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ
    """
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    gpu_memory = "N/A"
    if torch.cuda.is_available():
        gpu_memory = f"{torch.cuda.memory_allocated() / 1024**3:.2f}GB"
    
    return {'ram': f"{memory_info.rss / 1024**3:.2f}GB", 'gpu': gpu_memory}


def img_dataloader(data_dir_path):
    """
    ä»å·²æŒ‰ç±»åˆ«åˆ†å­æ–‡ä»¶å¤¹çš„ç›®å½•ä¸­åŠ è½½å›¾åƒè·¯å¾„å’Œæ ‡ç­¾
    """
    all_image_paths = []
    all_labels = []
    expected_labels_str = [str(i) for i in range(config.NUM_CLASSES)]
    label_to_idx = {label_str: idx for idx, label_str in enumerate(expected_labels_str)}
    
    if not os.path.isdir(data_dir_path):
        return [], []

    for label_str in expected_labels_str:
        class_dir = os.path.join(data_dir_path, label_str)
        if not os.path.isdir(class_dir):
            continue
        
        img_patterns = ["*.jpg", "*.JPG", "*.jpeg", "*.JPEG", "*.png", "*.PNG"]
        for pattern in img_patterns:
            for img_path in glob.glob(os.path.join(class_dir, pattern)):
                all_image_paths.append(img_path)
                all_labels.append(label_to_idx[label_str])

    return all_image_paths, all_labels


def create_weighted_sampler(labels):
    """
    åˆ›å»ºåŠ æƒé‡‡æ ·å™¨æ¥å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†ï¼Œè®©è®­ç»ƒæ—¶å„ä¸ªç±»åˆ«çš„æ ·æœ¬è¢«é€‰ä¸­çš„æ¦‚ç‡æ›´åŠ å‡è¡¡ã€‚
    """
    class_counts = Counter(labels)
    total_samples = len(labels)
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æƒé‡
    class_weights = {}
    for class_idx in range(config.NUM_CLASSES):
        if class_idx in class_counts:
            class_weights[class_idx] = total_samples / (config.NUM_CLASSES * class_counts[class_idx])
        else:
            class_weights[class_idx] = 1.0
    
    # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…æƒé‡
    sample_weights = [class_weights[label] for label in labels]
    
    return WeightedRandomSampler(weights=sample_weights, 
                                 num_samples=len(sample_weights), 
                                 replacement=True)


def train_one_epoch(model, dataloader, criterion, optimizer, device, accumulation_steps=1, epoch=None, scaler=None):
    """
    è®­ç»ƒä¸€ä¸ªepochï¼Œæ”¯æŒæ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦
    """
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    optimizer.zero_grad()  # åœ¨epochå¼€å§‹æ—¶æ¸…é›¶æ¢¯åº¦

    pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}' if epoch is not None else 'Training')
    for batch_idx, (inputs, labels) in enumerate(pbar):
        if inputs is None or labels is None:
            continue
            
        inputs, labels = inputs.to(device), labels.to(device)
        
        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        if scaler is not None:
            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                # ç¼©æ”¾æŸå¤±ä»¥åŒ¹é…ç´¯ç§¯æ­¥æ•°
                loss = loss / accumulation_steps
            
            # æ··åˆç²¾åº¦åå‘ä¼ æ’­
            scaler.scale(loss).backward()
        else:
            # æ ‡å‡†ç²¾åº¦å‰å‘ä¼ æ’­
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            # ç¼©æ”¾æŸå¤±ä»¥åŒ¹é…ç´¯ç§¯æ­¥æ•°
            loss = loss / accumulation_steps
            loss.backward()
        
        # ç´¯ç§¯æ¢¯åº¦
        if (batch_idx + 1) % accumulation_steps == 0:
            if scaler is not None:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.GRADIENT_CLIP_NORM)
                scaler.step(optimizer)
                scaler.update()
            else:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.GRADIENT_CLIP_NORM)
                optimizer.step()
            optimizer.zero_grad()
        
        running_loss += loss.item() * accumulation_steps  # æ¢å¤åŸå§‹æŸå¤±å€¼ç”¨äºè®°å½•
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
        # æ›´æ–°è¿›åº¦æ¡
        current_loss = running_loss / (batch_idx + 1)
        current_acc = 100 * correct / total
        pbar.set_postfix({'Loss': f'{current_loss:.4f}',
                          'Acc': f'{current_acc:.2f}%',
                          'Accum': f'{((batch_idx + 1) % accumulation_steps) + 1}/{accumulation_steps}'})
    
    # å¤„ç†æœ€åä¸€ä¸ªä¸å®Œæ•´çš„ç´¯ç§¯æ‰¹æ¬¡
    if len(dataloader) % accumulation_steps != 0:
        if scaler is not None:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
        else:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
        optimizer.zero_grad()
    
    avg_loss = running_loss / len(dataloader)
    accuracy = 100 * correct / total
    return avg_loss, accuracy


def validate(model, dataloader, criterion, device):
    """
    éªŒè¯æ¨¡å‹
    """
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, labels in dataloader:
            if inputs is None or labels is None:
                continue
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    avg_loss = running_loss / len(dataloader)
    accuracy = 100 * correct / total
    return avg_loss, accuracy


def save_checkpoint(model, optimizer, epoch, accuracy, best_accuracy, model_config, save_path):
    """
    ä¿å­˜æ£€æŸ¥ç‚¹
    """
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'accuracy': accuracy,
        'best_accuracy': best_accuracy,
        'model_config': model_config
    }, save_path)


def main():
    parser = argparse.ArgumentParser(description='Vis Training')
    parser.add_argument('--resume', type=str, help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--loss_type', type=str, choices=['crossentropy', 'focal'], default='crossentropy')
    parser.add_argument('--weighted_sampler', action='store_true', help='æ˜¯å¦ä½¿ç”¨åŠ æƒé‡‡æ ·å™¨')
    parser.add_argument('--weighted_loss', action='store_true', help='æ˜¯å¦åœ¨æŸå¤±å‡½æ•°ä¸­ä½¿ç”¨ç±»åˆ«æƒé‡')

    # æœ‰default
    parser.add_argument('--weight_mode', type=str, choices=['balanced', 'sqrt_balanced', 'log_balanced'], default=config.WEIGHT_MODE)
    parser.add_argument('--smooth_factor', type=float, default=config.SMOOTH_FACTOR)
    parser.add_argument('--focal_gamma', type=float, default=config.FOCAL_GAMMA)
    parser.add_argument('--focal_alpha', type=str, default=config.FOCAL_ALPHA)
    parser.add_argument('--label_smoothing', type=float, default=config.LABEL_SMOOTHING)
    parser.add_argument('--seed', type=int, default=3407, help='éšæœºç§å­')
    args = parser.parse_args()
    
    # è®¾ç½®å…¨å±€éšæœºç§å­
    set_seed(args.seed)
    
    os.makedirs(config.MODEL_OUTPUT_DIR, exist_ok=True)
    logger = setup_logging(config.MODEL_OUTPUT_DIR) # è®¾ç½®æ—¥å¿—
    tb_writer = SummaryWriter(log_dir=os.path.join(config.MODEL_OUTPUT_DIR, 'tensorboard')) # è®¾ç½® TensorBoard

    # åŠ è½½æ•°æ®å’Œåˆ›å»ºæ•°æ®é›†
    train_image_paths, train_labels = img_dataloader(config.TRAIN_DATA_ROOT)
    val_image_paths, val_labels = img_dataloader(config.VAL_DATA_ROOT)
    train_dataset = VisibilityDataset(train_image_paths, train_labels, is_train=True)
    val_dataset = VisibilityDataset(val_image_paths, val_labels, is_train=False)
    
    if args.weighted_sampler: # ä½¿ç”¨åŠ æƒé‡‡æ ·å™¨å¤„ç†ä¸å¹³è¡¡æ•°æ®
        sampler = create_weighted_sampler(train_labels)
    else:
        sampler = None

    # åœ¨Windowsç³»ç»Ÿä¸Šï¼Œä¸ºäº†é¿å…å¤šè¿›ç¨‹åºåˆ—åŒ–é—®é¢˜ï¼Œå¯ä»¥è®¾ç½®num_workers=0
    # æˆ–è€…ç¡®ä¿æ‰€æœ‰transforméƒ½æ˜¯å¯åºåˆ—åŒ–çš„ï¼ˆå·²ä¿®å¤Lambdaå‡½æ•°é—®é¢˜ï¼‰
    train_loader = DataLoader(train_dataset, 
                              batch_size=config.BATCH_SIZE, 
                              sampler=sampler,
                              shuffle=(sampler is None),
                              num_workers=0,  # Windowsä¸Šè®¾ä¸º0é¿å…å¤šè¿›ç¨‹é—®é¢˜
                              pin_memory=True,
                              persistent_workers=False,
                              worker_init_fn=worker_init_fn,
                              collate_fn=collate_fn_filter_none)

    val_loader = DataLoader(val_dataset, 
                            batch_size=config.BATCH_SIZE, 
                            shuffle=False,
                            num_workers=0,  # Windowsä¸Šè®¾ä¸º0é¿å…å¤šè¿›ç¨‹é—®é¢˜
                            pin_memory=True,
                            persistent_workers=False,
                            worker_init_fn=worker_init_fn,
                            collate_fn=collate_fn_filter_none)
    
    logger.info(f"éšæœºç§å­: {args.seed}")
    logger.info(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    logger.info(f"æ‰¹æ¬¡å¤§å°: {config.BATCH_SIZE}, æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}")
    logger.info(f"åŠ æƒç­–ç•¥: é‡‡æ ·å™¨={args.weighted_sampler}, æŸå¤±æƒé‡={args.weighted_loss}")
    if args.weighted_loss:
        logger.info(f"æƒé‡æ¨¡å¼: {args.weight_mode}, å¹³æ»‘å› å­: {args.smooth_factor}")
    logger.info(f"æŸå¤±å‡½æ•°: {args.loss_type}" + (f", gamma={args.focal_gamma}" if args.loss_type == 'focal' else ""))
    
    # åˆ›å»ºæ¨¡å‹
    logger.info("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
    model_kwargs = config.get_model_kwargs()    
    model = VisMFN(**model_kwargs)
    model_config = model_kwargs.copy()
        
    # é¿å…åŒé‡åŠ æƒçš„å»ºè®®ï¼šå¦‚æœä½¿ç”¨åŠ æƒé‡‡æ ·å™¨ï¼Œå»ºè®®ä¸ä½¿ç”¨æŸå¤±å‡½æ•°æƒé‡
    if args.weighted_sampler and args.weighted_loss:
        logger.warning("è­¦å‘Šï¼šåŒæ—¶ä½¿ç”¨åŠ æƒé‡‡æ ·å™¨å’ŒæŸå¤±å‡½æ•°æƒé‡å¯èƒ½å¯¼è‡´åŒé‡åŠ æƒï¼Œå»ºè®®åªä½¿ç”¨å…¶ä¸­ä¸€ç§")
    
    # å¤„ç†focal_alphaå‚æ•°
    try:
        focal_alpha = float(args.focal_alpha)
    except ValueError:
        focal_alpha = args.focal_alpha  # 'auto'ç­‰å­—ç¬¦ä¸²å€¼
    
    # losså’Œä¼˜åŒ–å™¨
    criterion = create_loss_function(
        train_labels, 
        loss_type=args.loss_type, 
        alpha=focal_alpha, 
        gamma=args.focal_gamma,
        use_weights=args.weighted_loss,
        weight_mode=args.weight_mode,
        smooth_factor=args.smooth_factor,
        label_smoothing=args.label_smoothing
        )
    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ä¼˜åŒ–å™¨å‚æ•°
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=config.LEARNING_RATE, 
        weight_decay=config.WEIGHT_DECAY, 
        betas=config.BETAS,
        eps=config.EPS)
    
    # å¸¦é¢„çƒ­çš„å­¦ä¹ ç‡è°ƒåº¦å™¨
    def get_lr_scheduler(optimizer, warmup_epochs, total_epochs, eta_min):
        def lr_lambda(epoch):
            if epoch < warmup_epochs:
                # é¢„çƒ­é˜¶æ®µï¼šçº¿æ€§å¢é•¿
                return config.WARMUP_FACTOR + (1.0 - config.WARMUP_FACTOR) * epoch / warmup_epochs
            else:
                # ä½™å¼¦é€€ç«é˜¶æ®µ
                cos_epoch = epoch - warmup_epochs
                cos_total = total_epochs - warmup_epochs
                return eta_min + (1 - eta_min) * 0.5 * (1 + np.cos(np.pi * cos_epoch / cos_total))
        
        return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    scheduler = get_lr_scheduler(optimizer, config.WARMUP_EPOCHS, config.EPOCHS, config.ETA_MIN / config.LEARNING_RATE)
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler(device='cuda') if torch.cuda.is_available() else None
    
    # è®­ç»ƒå˜é‡
    start_epoch = 0
    best_accuracy = 0.0
    
    # æ¢å¤è®­ç»ƒ
    if args.resume and os.path.exists(args.resume):
        logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {args.resume}")
        checkpoint = torch.load(args.resume, map_location=config.DEVICE)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        best_accuracy = checkpoint.get('best_accuracy', 0.0)
        logger.info(f"æ¢å¤è®­ç»ƒä»ç¬¬ {start_epoch} è½®å¼€å§‹ï¼Œå½“å‰æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.4f}")
    
    # å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
    model = model.to(config.DEVICE)
    logger.info(f"æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {config.DEVICE}")
    
    # åœ¨TensorBoardä¸­å¯è§†åŒ–ç½‘ç»œç»“æ„
    try:
        logger.info("æ­£åœ¨ç”Ÿæˆç½‘ç»œç»“æ„å›¾...")
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥å¼ é‡
        dummy_input = torch.randn(1, 3, config.TARGET_IMG_HEIGHT, config.TARGET_IMG_WIDTH).to(config.DEVICE)
        
        # ä½¿ç”¨add_graphæ·»åŠ ç½‘ç»œç»“æ„åˆ°TensorBoard
        tb_writer.add_graph(model, dummy_input)
        logger.info("ç½‘ç»œç»“æ„å›¾å·²æ·»åŠ åˆ°TensorBoard")
        logger.info(f"ğŸ“Š å¯åŠ¨TensorBoardæŸ¥çœ‹ç½‘ç»œç»“æ„ï¼štensorboard --logdir={os.path.join(config.MODEL_OUTPUT_DIR, 'tensorboard')}")
        
        # ç«‹å³åˆ·æ–°TensorBoard
        tb_writer.flush()
        
    except Exception as e:
        logger.warning(f"ç”Ÿæˆç½‘ç»œç»“æ„å›¾æ—¶å‡ºç°é”™è¯¯: {e}")
        logger.warning("è®­ç»ƒå°†ç»§ç»­è¿›è¡Œï¼Œä½†ç½‘ç»œç»“æ„å›¾æœªç”Ÿæˆ")
    
    # è®­ç»ƒå¾ªç¯
    logger.info("å¼€å§‹è®­ç»ƒ...")
    for epoch in range(start_epoch, config.EPOCHS):
        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, config.DEVICE, config.GRADIENT_ACCUMULATION_STEPS, epoch, scaler)
        val_loss, val_acc = validate(model, val_loader, criterion, config.DEVICE)
        scheduler.step() # update learning rate
        
        # ç»“æœè®°å½•
        tb_writer.add_scalar('Loss/Train', train_loss, epoch)
        tb_writer.add_scalar('Loss/Validation', val_loss, epoch)
        tb_writer.add_scalar('Accuracy/Train', train_acc, epoch)
        tb_writer.add_scalar('Accuracy/Validation', val_acc, epoch)
        tb_writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], epoch)
        
        # æ¯5ä¸ªepochè®°å½•æ¨¡å‹å‚æ•°åˆ†å¸ƒï¼ˆé¿å…è¿‡åº¦å ç”¨å­˜å‚¨ç©ºé—´ï¼‰
        if (epoch + 1) % 5 == 0:
            try:
                for name, param in model.named_parameters():
                    if param.grad is not None:
                        # è®°å½•å‚æ•°å€¼åˆ†å¸ƒ
                        tb_writer.add_histogram(f'Parameters/{name}', param.data, epoch)
                        # è®°å½•æ¢¯åº¦åˆ†å¸ƒ  
                        tb_writer.add_histogram(f'Gradients/{name}', param.grad.data, epoch)
                        # è®°å½•å‚æ•°èŒƒæ•°
                        tb_writer.add_scalar(f'ParamNorms/{name}', param.data.norm().item(), epoch)
                        tb_writer.add_scalar(f'GradNorms/{name}', param.grad.data.norm().item(), epoch)
            except Exception as e:
                logger.warning(f"è®°å½•å‚æ•°åˆ†å¸ƒæ—¶å‡ºç°é”™è¯¯: {e}")
        
        # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory_usage = get_memory_usage()
        logger.info(f'Epoch [{epoch+1}/{config.EPOCHS}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        logger.info(f'å†…å­˜ä½¿ç”¨ - RAM: {memory_usage["ram"]}, GPU: {memory_usage["gpu"]}')
        
        # å¼ºåˆ¶åˆ·æ–°æ—¥å¿—ç¼“å†²åŒºï¼Œç¡®ä¿å®æ—¶å†™å…¥
        for handler in logger.handlers:
            if hasattr(handler, 'flush'):
                handler.flush()
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_accuracy:
            best_accuracy = val_acc
            best_model_path = os.path.join(config.MODEL_OUTPUT_DIR, 'vis_mfn_best.pth')
            save_checkpoint(model, optimizer, epoch, val_acc, best_accuracy, model_config, best_model_path)
            logger.info(f'æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (å‡†ç¡®ç‡: {val_acc:.2f}%)')
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % 3 == 0:
            checkpoint_path = os.path.join(config.MODEL_OUTPUT_DIR, f'vis_mfn_epoch_{epoch+1}.pth')
            save_checkpoint(model, optimizer, epoch, val_acc, best_accuracy, model_config, checkpoint_path)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(config.MODEL_OUTPUT_DIR, 'vis_mfn_final.pth')
    save_checkpoint(model, optimizer, config.EPOCHS-1, val_acc, best_accuracy, model_config, final_model_path)
    
    # å…³é—­ TensorBoard writer
    tb_writer.close()
    logger.info("è®­ç»ƒå®Œæˆï¼")
    logger.info(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_accuracy:.2f}%")

if __name__ == '__main__':
    main()